{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d619e891",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # We can suppress the warnings\n",
    "# my twitter keys are stores in a file outside the git repository for security reasons\n",
    "config = dotenv_values(\"/Users/ambrosedesmond/CCT_Projects/Ambrose_MSC_DS_CA2/.env\")\n",
    "bearer_token = config[\"BEARER_TOKEN\"]\n",
    "# this is the location of the twitter api access , looking for the most recent tweets\n",
    "search_url = \"https://api.twitter.com/2/tweets/search/recent\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d990e37",
   "metadata": {},
   "source": [
    "# Set up and run API query. Save result to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3edf73c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set up the query we want to pass to api , is a json object\n",
    "    #   Im adding parameter -is = is not a retweet. i dont want the same retweets showing up in my data \n",
    "    # also pulling in the author id and name\n",
    "# Im using two query_params the first is without the next_token field for the first query   \n",
    "query_params1 = {\n",
    "    'query':'BREXIT farmers -is:retweet',\n",
    "    'max_results': '100',\n",
    "    'tweet.fields':'author_id',\n",
    "    'user.fields':'name',\n",
    "\n",
    "}\n",
    "# This second query is for the loop , as it requires a next_token.\n",
    "query_params2 = {\n",
    "    'query':'BREXIT farmers -is:retweet',\n",
    "    'max_results': '100',\n",
    "    'tweet.fields':'author_id',\n",
    "    'user.fields':'name',\n",
    "    'next_token' : 'abcd',\n",
    "}\n",
    "\n",
    "# my authorisation keys are used here for ther twitter API\n",
    "def bearer_oauth(r):\n",
    "    \"\"\" Function for using bearer token\"\"\"\n",
    "    r.headers['Authorization'] = f\"Bearer {bearer_token}\"\n",
    "    r.headers['User-Agent'] = \"v2RecentSerchPython\"\n",
    "    return r\n",
    "\n",
    "\n",
    "def connect_to_endpoint(url,params):\n",
    "    \"\"\" Function to connect to twitter API\"\"\"\n",
    "    response = requests.get(url,auth=bearer_oauth, params=params)\n",
    "    #responce.status_code 200 is good anything else is an error\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "def append_dict(adict,json_response):\n",
    "    for i in range(len(json_response['data'])):\n",
    "        try:\n",
    "            adict[json_response['data'][i]['author_id']] = json_response['data'][i]['text']\n",
    "\n",
    "        except IndexError:\n",
    "            print(\"We had an index error\")\n",
    "    return(adict)\n",
    "\n",
    "    \n",
    "def twit_call():\n",
    "    twit_dict={}\n",
    "    loop_count = 0\n",
    "    json_response = connect_to_endpoint(search_url, query_params1)\n",
    "\n",
    "    append_dict(twit_dict,json_response)\n",
    "    # while json_response['meta']['next_token']:\n",
    "    while True:\n",
    "        # pagenation is taking the next_token and feeding it through to next query\n",
    "        if 'next_token' in json_response['meta']: \n",
    "            next_t = json_response['meta']['next_token']\n",
    "            query_params2['next_token'] = next_t\n",
    "        else:\n",
    "            # this is the last page\n",
    "            break\n",
    "        json_response = connect_to_endpoint(search_url, query_params2)\n",
    "        append_dict(twit_dict,json_response)\n",
    "           \n",
    "    return(twit_dict)\n",
    "\n",
    "my_dict = twit_call()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0cba9ec4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert dictionary to dataframe\n",
    "df = pd.DataFrame.from_dict(my_dict, orient='index',columns=['tweets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3bc965e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(584, 1)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "daf20c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1030824102070808582</th>\n",
       "      <td>POST-BREXIT £1,000 FARMING PAYMENTS ‘TOO LITTL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3366508409</th>\n",
       "      <td>@waltjack71 @sainsburys @Tesco @1GarethWynJone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275219963775778816</th>\n",
       "      <td>@waltjack71 @mikegalsworthy @sainsburys @Tesco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48992513</th>\n",
       "      <td>#BrexitReality Post-#Brexit £1,000 farming pay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246437728679530496</th>\n",
       "      <td>More #BrexitLies \\nScheme supposed to replace ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                tweets\n",
       "1030824102070808582  POST-BREXIT £1,000 FARMING PAYMENTS ‘TOO LITTL...\n",
       "3366508409           @waltjack71 @sainsburys @Tesco @1GarethWynJone...\n",
       "1275219963775778816  @waltjack71 @mikegalsworthy @sainsburys @Tesco...\n",
       "48992513             #BrexitReality Post-#Brexit £1,000 farming pay...\n",
       "1246437728679530496  More #BrexitLies \\nScheme supposed to replace ..."
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0daa413c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe as cvs file.\n",
    "#df.to_csv('BREXIT_tweets.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eef49bd",
   "metadata": {},
   "source": [
    "# Load previously saved tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7b207d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(\"BREXIT_tweets_5_jan.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2181e320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1030824102070808582</th>\n",
       "      <td>POST-BREXIT £1,000 FARMING PAYMENTS ‘TOO LITTL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3366508409</th>\n",
       "      <td>@waltjack71 @sainsburys @Tesco @1GarethWynJone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1275219963775778816</th>\n",
       "      <td>@waltjack71 @mikegalsworthy @sainsburys @Tesco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48992513</th>\n",
       "      <td>#BrexitReality Post-#Brexit £1,000 farming pay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246437728679530496</th>\n",
       "      <td>More #BrexitLies \\nScheme supposed to replace ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                tweets\n",
       "1030824102070808582  POST-BREXIT £1,000 FARMING PAYMENTS ‘TOO LITTL...\n",
       "3366508409           @waltjack71 @sainsburys @Tesco @1GarethWynJone...\n",
       "1275219963775778816  @waltjack71 @mikegalsworthy @sainsburys @Tesco...\n",
       "48992513             #BrexitReality Post-#Brexit £1,000 farming pay...\n",
       "1246437728679530496  More #BrexitLies \\nScheme supposed to replace ..."
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "70255eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b70dee42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ambrosedesmond/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9feb4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using regular expressions to remove all special charecters.\n",
    "df[\"tweets\"] = df['tweets'].str.replace('[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "15823825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using regular expressions to remove all tabs and carrige returns that were imported in original csv\n",
    "df[\"tweets\"] = df['tweets'].str.replace(r'\\r\\n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d0f2ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basically stop words like this, an, a, the, etc that do not affect the meaning of the tweet will be removed\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6c9b2de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda counts the  number of stop words in each tweet\n",
    "df[\"stopwords\"]  = df[\"tweets\"].apply(lambda x : len([x for x in x.split() if x in stop]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d02834a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda counts the number of upper case words in tweet , indicate shouting , anger ect\n",
    "df[\"upper\"]  = df[\"tweets\"].apply(lambda x : len([x for x in x.split() if x.isupper()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2bd01fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a1b5b223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 583 entries, 3366508409 to 1282693762385403906\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   tweets     583 non-null    object\n",
      " 1   stopwords  583 non-null    int64 \n",
      " 2   upper      583 non-null    int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 18.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "86c1f88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweets'] = df['tweets'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f357ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### Warning this cell can take up to 20 mins to complete ################### \n",
    "# corrects spellings and grammer based on context\n",
    "# use txtblob to correct the tweet spelling\n",
    "from textblob import TextBlob\n",
    "df['tweets_correct?']= df[\"tweets\"].apply(lambda x : str(TextBlob(x).correct()))\n",
    "# pattern matching is not good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23bb45a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TextBlob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# get the sentiment . using scored words. scores are added up and the final score is the entimenen \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# The polarity score is a float within the range [-1.0, 1.0]. The subjectivity is a float within the \u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective.\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment_1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtweets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mTextBlob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentiment\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment_2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtweets_correct?\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x : TextBlob(x)\u001b[38;5;241m.\u001b[39msentiment[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4324\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4328\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4329\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4330\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4331\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4332\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4431\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4432\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py:1082\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;66;03m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[1;32m   1080\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m-> 1082\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py:1137\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;66;03m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;66;03m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[0;32m-> 1137\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1140\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1144\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# get the sentiment . using scored words. scores are added up and the final score is the entimenen \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# The polarity score is a float within the range [-1.0, 1.0]. The subjectivity is a float within the \u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective.\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment_1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtweets\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x : \u001b[43mTextBlob\u001b[49m(x)\u001b[38;5;241m.\u001b[39msentiment[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      5\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment_2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtweets_correct?\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x : TextBlob(x)\u001b[38;5;241m.\u001b[39msentiment[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TextBlob' is not defined"
     ]
    }
   ],
   "source": [
    "# get the sentiment . using scored words. scores are added up and the final score is the entimenen \n",
    "# The polarity score is a float within the range [-1.0, 1.0]. The subjectivity is a float within the \n",
    "# range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective.\n",
    "df['sentiment_1'] = df[\"tweets\"].apply(lambda x : TextBlob(x).sentiment[0])\n",
    "df['sentiment_2'] = df[\"tweets_correct?\"].apply(lambda x : TextBlob(x).sentiment[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64df5169",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81291e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by='sentiment_1', ascending=False)\n",
    "df = df.sort_values(by='sentiment_2', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3f5e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the histogram gives a instant view of all sentiment , and changes from day to day\n",
    "# depending on the news topic of the day.\n",
    "df.hist(column='sentiment_1')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51da3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The histogram give a visual indication of the sentiment, \n",
    "# we see the majority of the sentiment is around 0 on the x axis indicating neutral sentiment \n",
    "# in relation to Brexit and farming on the day the reading was taken. Previous readings \n",
    "# have tended to be mostly negative.\n",
    "df.hist(column='sentiment_2')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235dff56",
   "metadata": {},
   "source": [
    "## BAG OF WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a99ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the comments\n",
    "X = df[\"tweets\"]\n",
    "y = df[\"sentiment_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ffd4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2883e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lamdba to categorise the sentiment to 0,1,2 groupings\n",
    "y = y.apply(lambda x: 0 if x < -0.5 else (1 if x < 0.5 else 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802034ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da0b617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import string\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057b7e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the stopwords into the object named as \"stop_words\"\n",
    "# Stopwords are words which do not contain enough significance to be used without our algorithm\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Store the string.punctuation into an object punct. Removing Punctuation: ‘,.*!’ and other \n",
    "# punctuation marks that are not really needed by the model\n",
    "punct = string.punctuation\n",
    "\n",
    "# Initialise an object using a method PorterStemmer.\n",
    "# reducing words like ‘jumping, jumped, jump’ into its root word(also called stem)\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c81436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "cleaned_data=[]\n",
    "\n",
    "# For loop from first value to length(X), ^a-zA-Z means include small and capital case letters\n",
    "\n",
    "for i in range(len(X)):\n",
    "    message = re.sub('[^a-zA-Z]', ' ', X.iloc[i])\n",
    "    message = message.lower().split()\n",
    "    message = [stemmer.stem(word) for word in message if (word not in stop_words) and (word not in punct)]\n",
    "    # we rejoin all the words back into a sentance again and append the sentances into a list\n",
    "    message = ' '.join(message)\n",
    "    cleaned_data.append(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb3f401",
   "metadata": {},
   "outputs": [],
   "source": [
    "X # this is the regular cleaned data, note I have full sentices with all the unnecessary words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1a5428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unnecessary (stop words) removed but meaning still intact. the stemer sometimes abbreviates words too\n",
    "cleaned_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33c6615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization is the act of breaking up a sequence of strings into pieces such \n",
    "# as words, keywords, phrases, symbols and other elements called tokens\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Instantiate an object cv by calling a method named as CountVectorzer()\n",
    "# here we are only taking 3K columns else the dataset will be too big\n",
    "# one con here is the order of the words is lost so the meaning can be lost a little\n",
    "# set stop words to names that dont indicate sentiment\n",
    "cv    = CountVectorizer(max_features = 3000, stop_words = ['BREXIT'])\n",
    "\n",
    "# convert the cleaneddata into the bag of words we want\n",
    "X_fin = cv.fit_transform(cleaned_data).toarray()\n",
    "\n",
    "# Display the rows and colums\n",
    "X_fin.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4929a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the feature vector just created\n",
    "X_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce9cf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the NLP model using the input and the output, use the Multinomial Naive Bayes\n",
    "# model to figure out the relationship between the input and the output.\n",
    "# its a supervised learning algorithm that works really well for text based data\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Instantiate an object model by calling a method MultinomialNB()\n",
    "model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21417d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_fin, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc8e5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model by calling a method fit()\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1218710e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call predict() method\n",
    "# To check how good our model is we first make predictions on the test set\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca625ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4415aa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Instantiate a mthod named as Cla\n",
    "cf = classification_report(y_test, y_pred)\n",
    "\n",
    "# Display the values of an object cf\n",
    "print(cf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76494e68",
   "metadata": {},
   "source": [
    "## Feture Generation using TI-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c24fbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create an object 'tf' by calling a method TfidfVectorizer()\n",
    "tfidf = TfidfVectorizer(max_features = 3000)\n",
    "\n",
    "# Train the dataset by calling a method fit_tranform() \n",
    "X_tfidf = tfidf.fit_transform(cleaned_data).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bf54cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Instantiate an object model by calling a method MultinomialNB()\n",
    "model_tdidf = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4d53a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbd2b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model by calling a method fit()\n",
    "model_tdidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bc1b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call predict() method\n",
    "y_pred = model_tdidf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b787cd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Instantiate a mthod named as Cla\n",
    "cf = classification_report(y_test, y_pred)\n",
    "\n",
    "# Display the values of an object cf\n",
    "print(cf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430b8de0",
   "metadata": {},
   "source": [
    "## Time series on milk production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fd10d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow / Keras\n",
    "from tensorflow import keras # for building Neural Networks\n",
    "print('Tensorflow/Keras: %s' % keras.__version__) # print version\n",
    "from keras.models import Sequential # for creating a linear stack of layers for our Neural Network\n",
    "from keras import Input # for instantiating a keras tensor\n",
    "from keras.layers import Dense, SimpleRNN # for creating regular densely-connected NN layers and RNN layers\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd # for data manipulation\n",
    "print('pandas: %s' % pd.__version__) # print version\n",
    "import numpy as np # for data manipulation\n",
    "print('numpy: %s' % np.__version__) # print version\n",
    "import math # to help with data reshaping of the data\n",
    "\n",
    "# Sklearn\n",
    "import sklearn # for model evaluation\n",
    "print('sklearn: %s' % sklearn.__version__) # print version\n",
    "from sklearn.model_selection import train_test_split # for splitting the data into train and test samples\n",
    "from sklearn.metrics import mean_squared_error # for model evaluation metrics\n",
    "from sklearn.preprocessing import MinMaxScaler # for feature scaling\n",
    "\n",
    "# Visualization\n",
    "import plotly \n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "print('plotly: %s' % plotly.__version__) # print version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8227ce36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
